

6x6x6 in the repo
=================
DONE - step20 is a little slow...this is one of those 540 billion IDA tables where the prune tables are only 735k each
    The one checked in is 6-deep and 29M zipped but the 7-deep one is 579M zipped.  Could add part of the 7-deep table
    to get it up to 100M?

    FIX: I changed the code so it solves the left_only prune table first to try to speed things up, if that doesn't work
    then solve the right_only prune table.  Typically one of them will speed up the IDA search a great deal.

This one takes 11s...I'm not sure what else to change to speed it up other than build a deeper primary table
What if we built a table to move the UD obliques from 3 sides to their final position?
xxxxxxxDRRLxxLDDBxxLUUDxxFRDUxxxxxxxxxxxxxxBBLBxxURFUxxDRBDxxDFDLxxxxxxxxxxxxxxULLRxxUFLLxxBLFRxxBBRDxxxxxxxxxxxxxxLFBRxxBUUFxxFDDFxxURUFxxxxxxxxxxxxxxRFDLxxURFUxxUBBFxxRULDxxxxxxxxxxxxxxBBLFxxFLLRxxDRBBxxFDRUxxxxxxx

DONE - step60 is a little slow
    This is 9-deep with 25 million entries and is 25M zipped.  Build this to 7-deep and check in part of it.

    FIX: I fixed this by calling self.lt_LR_solve_inner_x_centers_and_oblique_edges.solve() and then removing
    the L, L', L2, R, R', R2 turns when I call self.lt_LFRB_solve_inner_x_centers_and_oblique_edges.solve().
    This is the same strategy I used for 7x7x7


7x7x7 in the repo
=================
- lookup-table-7x7x7-step50-UD-solve-inner-center-and-oblique-edges.txt is 2.4G
    - this currently has all steps
    - it is built 9-deep...could we get by with 8-deep?

    - The 8-deep last-step-only build is only 26M zipped, it looks like this:

dwalton@laptop ~/l/rubiks-cube-lookup-tables> tail lookup-table-7x7x7-step50-UD-solve-inner-center-and-oblique-edges.txt.8-deep
xUUUxUUUUUUUUUUUUUUUxUDDxxUUDxDDDDDDDDDDDDDDDxDDDx:U
xUUUxUUUUUUUUUUUUUUUxUDUxxDDDxDDDDDDDDDDDDDDDxDUDx:D2
xUUUxUUUUUUUUUUUUUUUxUDUxxDDDxDDDDDDDDDUDDDDDxDDDx:U'

    - If I don't store the corners there will be no Xs so then I could store it
      as hex which would be much smaller...maybe I could get the 9-deep below 100M then.
      The cube state would be 11 characters instead of 50..so save 39 bytes per line.
      The current 8-deep table is 274,307,432 bytes  over 4,898,347 lines....so 39 * 4,898,347
      is 191,035,533 so the file would be 83,271,899 or 17 bytes per entry on average. I know
      the 9-deep table would have 27,156,823 lines so that would be 461,665,991...that should
      compress down to below 100M.


- step70 needs some work
    lookup-table-7x7x7-step70-LFRB-solve-inner-center-and-oblique-edges.txt.10-deep is 62G...build a 9-deep copy of this
    step71, 72, and 73 prune tables all have 24 million entries and are 3.7G...we could split each of these
    up into two 4900 entry tables...those would be very small

    8-deep is 71M zipped, 1.2G unzipped so there is no way a 9-deep table will fit in the repo...I just need to try
    this and see how slow the IDA is.


- the step60 table is storing FB but they are all Xs which seems a little silly. This table isn't big at all though so
  not sure it is worth the savings to fix this.


Misc
====

DONE - do NoIDASolution stuff for 777

DONE - go ahead and do the rest of the 777 solver even though centers are not done yet

DONE - Even cubes...we need to prevent OLL when we figure out that we will hit it later

DONE - Avoid PLL parity for 6x6x6...not 100% sure how to do this.  Maybe only look for
       it when pairing the outer orbit of edges.

- build lookup-table-5x5x5-step20-LR-centers-stage.txt out all the way (165 million entries)

- pair_one_edge_555() is broken if you don't pair all of the outside edges first via 444

- 4x4x4-edges and 5x5x5-edges. We need to see what "cycles" of edges there are and for
the edges that are not part of the cycle x them out as if they were paired and then do
the lookup. Say for instance in a 444 there are only two edges at play, that solution
is for sure in the table but we have to x out all of the other edges in order for us to
find it.

How to do this?

Brute force...worst case say there are 12 unpaired edges, we need to try Xing out every
combination with 1 edge, then 2 edges, then 3 edges, etc all the way up to Xing out 11 edges.

    foo = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l')

    total_count = 0
    for x in range(1, 12):
        combos = itertools.combinations(foo, x)
        count = 0
        for combo in combos:
            count += 1
        total_count += count
        print("With %d edges there are %d combos" % (x, count))
    print("%d total combos" % total_count)

Tells us
    With 1 edges there are 12 combos
    With 2 edges there are 66 combos
    With 3 edges there are 220 combos
    With 4 edges there are 495 combos
    With 5 edges there are 792 combos
    With 6 edges there are 924 combos
    With 7 edges there are 792 combos
    With 8 edges there are 495 combos
    With 9 edges there are 220 combos
    With 10 edges there are 66 combos
    With 11 edges there are 12 combos
    4094 total combos

Looking up 4094 at one time wouldn't be that bad. If we get multiple hits we need to keep the
one that pairs the most edges...that should be the entry where we Xed out the fewest edges.
